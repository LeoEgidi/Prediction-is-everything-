Temptative titles:  
Prediction isn't everything, but everything is prediction
Prediction is nothing without control

POSSIBLE OUTLINE

1. Introduction
2. The role of prediction in statistics
3. The role of prediction in ML
4. A tentative comparison
5. Applied examples
6. Conclusions

GENERAL IDEAS

- Statistics has always been thought as the "science of inference", or "science of the estimates", but rarely
 statisticians face the idea to validate their models on the ground of predictive accuracy (or prediction)
- Conversely, machine learning is the "science of the prediction", and rarrely machine learners try
 to explain how this predictive power has been gained.
- In terms of metaphors, the 'two-cultures' of Leo Breiman may be seen as follows:
    - statisticians are very elegant in using their guns, and they perfectly know how 
      to use them, how they are built, how much precision they may obtain
    - machine learners are very aggressive with their bazooka, they only have a 
      rough idea how they are built, they use them to 'kill the rabbit'.
- ML techniques are often not generative about the problem of interest.
- ML techniques often rely on the so called 'training-sample shaking'.
- The role of overfitting in prediction
- We should investigate the role of prediction in distinct areas of statistics: supervised and unsupervised
 learning. But also regression and classification
- Softwares like Stan, which are transparent and make clear the use of the generated quantities, are the best
 predictive-oriented statistical tools.

COMMUNICATION DUTIES 

- Statisticians should remark the appealing idea of predictive accuracy based on uncertainty, such as the
posterior predictive distribution for future data. Point predictions are dangerous
- ML also use a sort of underlying uncertainty, by using B trees in Random forests or in Bagging. This uncertainty is
often not so transparent.
- There is an ongoing debate about the supremacy of prediction over accomodation: quickly, is a theory which predicts new items
 better than a theory designed 'just' to describe the current data? The so called 'predictivism' philosophy (Maher, Worrall) 
 states this supremacy. Accomodation is often seen to degenerate in overfitting (Hitchcok and Sober)
- However, only statistics (and not ML) makes clear how to predict and forecast, having an underlying data mechanism.
 Generative models are useful also for prediction!

SOME QUESTIONS, SOME TEMPTATIVE ANSWERS

  (a) Is the prediction part of a science design? 
  
      Using Bertrand Russel's words, I mean with science design the following scheme:

      1) hypothesis formulation
      2) Data collection
      3) Validation of the hypothesis through data

      and this is quite the the same as the rules suggested by Andrew Gelam about Bayesian inference foundations, when he
      says:

      "The process of Bayesian data analysis can be idealized by dividing it into the following three steps:

	1. Setting up a full probability model—a joint probability distribution for all observable and unobservable quantities 
           in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection 
           process.

        2. Conditioning on observed data: calculating and interpreting the appropriate posterior distribution—the conditional probability 
           distribution of the unobserved quantities of ultimate interest, given the observed data.

	3. Evaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the 
           data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? 
           In response, one can alter or expand the model and repeat the three steps.
         "
     In such a view, statistical science is deeply scientific and inductive. And according to this,
     prediction task is far away from the scientific practice.
     (I would say, that statistics may be seen as the 'skeleton of science')

  (b) Then, can we really state that machine learning procedures and Artificial Intelligence algorithms are valid and 
      good scientific tools only because they 'predict well'?
  
      Out-of-sample data are observable, not observed, so we should not truly trust the plausibility of our
      predictions, since they 'cannot fit the out-of-sample' data. Moreover, as Popper says, 'every scientific theory should
      be falsifiable': how could we falsify a theory if we do not have the data yet?

  (c) Is prediction useful in social science applications (presidential elections, economics measurements, political choices, etc.)?

      Karl Popper says that the idea of predicting social science events is often useless. However, produce predictions does not
      seem that we cannot do predictions.

  (d) Model checking and AI, Andrew's blog (https://statmodeling.stat.columbia.edu/2016/11/21/) + Lake et al. (2016)

SOME INITIAL REFERENCES

 - Gelman, Popper, Breiman, Mayo, Russell, Hennig, Bzdok, Taleb, Maher


